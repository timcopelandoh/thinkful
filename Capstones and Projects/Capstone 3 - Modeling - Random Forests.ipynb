{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "loan = pd.read_csv('accepted_2007_to_2018Q4.csv')\n",
    "\n",
    "return_number = {\n",
    "    'Jan': 1,\n",
    "    'Feb': 2,\n",
    "    'Mar': 3,\n",
    "    'Apr': 4,\n",
    "    'May': 5,\n",
    "    'Jun': 6,\n",
    "    'Jul': 7,\n",
    "    'Aug': 8,\n",
    "    'Sep': 9,\n",
    "    'Oct': 10,\n",
    "    'Nov': 11,\n",
    "    'Dec': 12\n",
    "}\n",
    "\n",
    "def convert_date(d):\n",
    "    try:\n",
    "        return datetime.date(year=int(d[4:]), month=return_number[d[:3]], day=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "loan['issue_d'] = loan['issue_d'].apply(convert_date)\n",
    "loan['earliest_cr_line'] = loan['earliest_cr_line'].apply(convert_date)\n",
    "\n",
    "loan = loan[loan.issue_d < datetime.date(2015,7,1)]\n",
    "loan = loan[loan.issue_d >= datetime.date(2010,1,1)]\n",
    "\n",
    "loan = loan[loan.term == ' 36 months']\n",
    "\n",
    "loan['earliest_cr_line'] = loan.apply(lambda x: (x['issue_d'] - x['earliest_cr_line']).days, axis=1)\n",
    "\n",
    "loan = loan[(loan.loan_status == 'Fully Paid') | (loan.loan_status == 'Charged Off')]\n",
    "\n",
    "loan = loan[loan.annual_inc < 1000000]\n",
    "\n",
    "loan['ln_annual_inc'] = np.log(loan.annual_inc)\n",
    "\n",
    "loan = loan[loan.revol_util < 150]\n",
    "loan['ln_revol_bal'] = np.log(loan.revol_bal+1)\n",
    "\n",
    "loan['ln_earliest_cr_line'] = np.log(loan.earliest_cr_line)\n",
    "\n",
    "loan['ln_open_acc'] = np.log(loan.open_acc)\n",
    "\n",
    "loan.rename(columns = {'delinq_2yrs': 'num_delinq_2yrs'}, inplace=True)\n",
    "loan['delinq_2yrs'] = (loan['num_delinq_2yrs'] >= 1)\n",
    "\n",
    "loan.rename(columns = {'pub_rec': 'num_pub_rec'}, inplace=True)\n",
    "loan['pub_rec'] = (loan['num_pub_rec'] >= 1)\n",
    "\n",
    "loan.rename(columns = {'inq_last_6mths': 'num_inq_last_6mths'}, inplace=True)\n",
    "loan['inq_last_6mths'] = (loan['num_inq_last_6mths'] >= 1)\n",
    "\n",
    "loan['target'] = (loan['loan_status'] == 'Fully Paid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['loan_amnt', 'int_rate', 'ln_annual_inc', 'dti', 'fico_range_high', 'num_delinq_2yrs', 'ln_earliest_cr_line', 'num_inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'ln_open_acc', 'num_pub_rec', 'ln_revol_bal', 'revol_util', 'total_acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 2010-2013 to tune our models, and save data from 2014 and the first \n",
    "# 6 months of 2015 for walk-forward testing\n",
    "\n",
    "train = loan[loan.issue_d < datetime.date(2014, 1, 1)]\n",
    "test = loan[(loan.issue_d >= datetime.date(2014, 1, 1)) & (loan.issue_d < datetime.date(2015, 7, 1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "X1 = train[['loan_amnt', 'ln_annual_inc', 'dti', 'fico_range_high', 'delinq_2yrs', 'num_delinq_2yrs', 'ln_earliest_cr_line', 'inq_last_6mths', 'num_inq_last_6mths', 'ln_open_acc', 'pub_rec', 'num_pub_rec', 'ln_revol_bal', 'revol_util', 'total_acc']]\n",
    "X2 = pd.concat([X1, train.int_rate, pd.get_dummies(train.grade)], axis=1)\n",
    "X3 = pd.concat([X1, train.int_rate, pd.get_dummies(train.sub_grade)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = test[['loan_amnt', 'ln_annual_inc', 'dti', 'fico_range_high', 'delinq_2yrs', 'num_delinq_2yrs', 'ln_earliest_cr_line', 'inq_last_6mths', 'num_inq_last_6mths', 'ln_open_acc', 'pub_rec', 'num_pub_rec', 'ln_revol_bal', 'revol_util', 'total_acc']]\n",
    "X2_test = pd.concat([X1_test, test.int_rate, pd.get_dummies(test.grade)], axis=1)\n",
    "X3_test = pd.concat([X1_test, test.int_rate, pd.get_dummies(test.sub_grade)], axis=1)\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8761131125804026\n",
      "75.21725583076477\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "rfc1 = ensemble.RandomForestClassifier(max_depth=5, max_features=5, n_estimators=50)\n",
    "scores = cross_val_score(rfc1, X1, y, cv=10)\n",
    "time_elapsed = time.time()-start\n",
    "print(scores.mean())\n",
    "print(time_elapsed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc1.fit(X1, y)\n",
    "\n",
    "\n",
    "for i in range(X1.shape[1]):\n",
    "    print('{}: {}'.format(X1.columns[i], rfc1.feature_importances_[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = rfc1.predict_proba(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([x[1] for x in pred1], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pred1'] = [x[1] for x in pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pct_return'] = (test.total_pymnt - test.funded_amnt) / test.funded_amnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "test['pct_return'] = (test.total_pymnt - test.funded_amnt) / test.funded_amnt\n",
    "\n",
    "for letter in letters:\n",
    "    thresh_m1_80 = np.percentile(test[test.grade == letter]['pred1'], q=80)\n",
    "\n",
    "    thresh_m1_90 = np.percentile(test[test.grade == letter]['pred1'], q=90)\n",
    "\n",
    "    thresh_m1_95 = np.percentile(test[test.grade == letter]['pred1'], q=95)\n",
    "    \n",
    "    thresh_m1_99 = np.percentile(test[test.grade == letter]['pred1'], q=99)\n",
    "\n",
    "    print('-----------------  ' + letter + '  -----------------')\n",
    "    print('')\n",
    "    \n",
    "    print('Naive Returns:       {}'.format(test[test.grade==letter]['pct_return'].mean()*100))\n",
    "    \n",
    "    print('')\n",
    "    print('Model 1:')\n",
    "    print('>80th percentile:    {}'.format(test[(test.grade==letter)&(test.pred1 >= thresh_m1_80)]['pct_return'].mean()*100))\n",
    "    print('>90th percentile:    {}'.format(test[(test.grade==letter)&(test.pred1 >= thresh_m1_90)]['pct_return'].mean()*100))\n",
    "    print('>95th percentile:    {}'.format(test[(test.grade==letter)&(test.pred1 >= thresh_m1_95)]['pct_return'].mean()*100))\n",
    "    print('>99th percentile:    {}'.format(test[(test.grade==letter)&(test.pred1 >= thresh_m1_99)]['pct_return'].mean()*100))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "par am_grid = {\n",
    "    'max_depth': [1, 2, 3, 5, 10, 20, 40],\n",
    "    'max_features': [2, 3, 4, 5],\n",
    "    'n_estimators': [10, 50, 100, 500]\n",
    "}\n",
    "\n",
    "param_grid2 = {\n",
    "    'max_depth': [2,3,5],\n",
    "    'max_features': [2,3],\n",
    "    'n_estimators': [50]\n",
    "}\n",
    "\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "\n",
    "start = time.time()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid = param_grid, cv=3)\n",
    "grid_search.fit(X1, y)\n",
    "elapsed = time.time()-start\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print('{} seconds'.format(round(elapsed, 0)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1,2,3,5,7,10,15,20,25,30,40,50,70,90],\n",
    "    'max_features': [2,3,4,5,7,10,15],\n",
    "    'n_estimators': [50]\n",
    "}\n",
    "\n",
    "rf2 = ensemble.RandomForestClassifier()\n",
    "\n",
    "rf2_random = RandomizedSearchCV(estimator=rf2, param_distributions = param_grid, n_iter=40, cv=5)\n",
    "\n",
    "rf2_random.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf3 = ensemble.RandomForestClassifier(n_estimators=50, max_features=3, max_depth=10)\n",
    "\n",
    "rf3.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['pred2'] = [x[1] for x in rf3.predict_proba(X1_test)]\n",
    "\n",
    "def returns_by_grade(metric, grade, returns, thresh = [80, 90, 95]):\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n",
    "        \n",
    "        cutoffs = [np.percentile(metric[grade == letter], q=x) for x in thresh]\n",
    "        \n",
    "        print('--------------  ' + letter + '  --------------')\n",
    "        print('')\n",
    "    \n",
    "        print('Naive Returns:             {}%'.format(round(returns[grade==letter].mean()*100, 2)))\n",
    "        print('')\n",
    "        \n",
    "        for c in range(len(thresh)):\n",
    "            print('{} Percent Cutoff:         {}%'.format(thresh[c], round(returns[(grade == letter) & (metric >= cutoffs[c])].mean()*100,2)))\n",
    "            \n",
    "        print('')\n",
    "        \n",
    "returns_by_grade(test.pred2, test.grade, test.pct_return)\n",
    "\n",
    "#test[['pred2', 'grade', 'pct_return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(test.target, test.pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(test.target, test.pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model with optimized hyperparameters did improve the model somewhat over manual selection of parameteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf4 = ensemble.RandomForestClassifier(max_depth=5, max_features=5, n_estimators=50)\n",
    "rf4.fit(X2, y)\n",
    "\n",
    "test['pred3'] = [x[1] for x in rf4.predict_proba(X2_test)]\n",
    "\n",
    "roc_auc_score(test.target, test.pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1,2,3,5,7,10,15,20,25,30,40,50,70,90],\n",
    "    'max_features': [2,3,4,5,7,10,15],\n",
    "    'n_estimators': [50]\n",
    "}\n",
    "\n",
    "rf5 = ensemble.RandomForestClassifier()\n",
    "\n",
    "rf5_random = RandomizedSearchCV(estimator=rf5, param_distributions = param_grid, n_iter=40, cv=3)\n",
    "\n",
    "rf5_random.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf2_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf6 = ensemble.RandomForestClassifier(n_estimators=500, max_features=4, max_depth=10)\n",
    "\n",
    "rf6.fit(X2, y)\n",
    "\n",
    "test['pred4'] = [x[1] for x in rf6.predict_proba(X2_test)]\n",
    "\n",
    "roc_auc_score(test.target, test.pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def graph_by_grade(predictions, grade, target):\n",
    "    pred = []\n",
    "    act = []\n",
    "    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(2,4,1)\n",
    "    plt.title(\"Overall\")\n",
    "    \n",
    "    for i in range(21):\n",
    "        \n",
    "        lb = .58 + i*.02\n",
    "        ub = .58 + (i+1)*.02\n",
    "        sub = ((predictions >= lb)&(predictions < ub))\n",
    "        pred_compliance = predictions[sub].mean()\n",
    "        act_compliance = target[sub].mean()\n",
    "        pred.append(pred_compliance)\n",
    "        act.append(act_compliance)\n",
    "\n",
    "    plt.plot(pred, act)\n",
    "    plt.plot(pred, pred)\n",
    "\n",
    "    for letter in range(7):\n",
    "        \n",
    "        pred = []\n",
    "        act = []\n",
    "        ss = []\n",
    "        \n",
    "        for i in range(21):\n",
    "            lb = .58 + i*.02\n",
    "            ub = .58 + (i+1)*.02\n",
    "            sub = ((predictions >= lb)&(predictions < ub)&(grade==letters[letter]))\n",
    "            pred_compliance = predictions[sub].mean()\n",
    "            act_compliance = target[sub].mean()\n",
    "            pred.append(pred_compliance)\n",
    "            act.append(act_compliance)\n",
    "\n",
    "        plt.subplot(2,4,letter+2)\n",
    "        plt.title('Loan Grade: {}'.format(letters[letter]))\n",
    "        plt.plot(pred, act)\n",
    "        plt.plot(pred, pred)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "graph_by_grade(test.pred4, test.grade, test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "returns_by_grade(test.pred4, test.grade, test.pct_return, [80, 90, 95, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
