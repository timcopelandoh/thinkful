{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for Neural Networks - Guided Example\n",
    "\n",
    "Here we're going to work through a classic Machine Learning problem - digit recognition. This data is referred to as the MNIST dataset, which stands for Modified National Institute of Standards and Technology, and represents probably the most used dataset in the world for advanced machine learning techniques (though the iris dataset would be a close second). Here we're forgoing a more business focused dataset for a few reasons. Firstly, this dataset is the most written about dataset in these topics - you'll easily find other guides using pure TensorFlow or other tools like Theano to solve the same problem with the same class of models. Similarly, you can also easily find several different kinds of neural networks being used to solve this problem. This will be valuable as you try to expand your knowledge of different kinds of layers and combinations.\n",
    "\n",
    "We'll be building our code off of the examples provided in the Keras documentation, and found in full on its [creator's github](https://github.com/fchollet/keras/tree/master/examples). \n",
    "\n",
    "Our goal here will be simple but multifaceted. Overall we are going to use the MNIST dataset and neural networks to classify handwritten numbers as the proper digits. This will be thought of as a multi-class classification problem, specifically with 10 classes (one for each possible digit).\n",
    "\n",
    "However, we will use this to teach a few new kinds of neural network compositions, creating three different styles of network and discussing their relative advantages and disadvantages. Through this we will delve a little deeper into neural network theory.\n",
    "\n",
    "But before we go too far, let's actually look at the data.\n",
    "\n",
    "## MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "type(np.array([0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at this data you'll notice its organization structure is not images. We don't actually see any pictures of digits here. Instead, what we have is values of pixels, a simple way of converting images into numeric data on which we can train a model.\n",
    "\n",
    "However, this still doesn't look like most of the data we've worked with previously. It's not a single table, but rather a different, higher dimensionality structure. It is often described as a set of clouds, each cloud representing an image. The cloud contains columns of values, representing the darkness of pixels. That's great, but not an easy or meaningful dataset on which to directly train a model. The darkness of the second pixel in the third column isn't likely linearly related to likelihood the cloud represents a certain digit. Instead, we need to find meaningful patterns within our clouds, creating models off of those patterns.\n",
    "\n",
    "This is exactly what neural networks are good at. Multiple layers will allow us to transform this clouds full of values into meaningful vectors containing the information we need to be able to create a model, admittedly in an unlabeled and unsupervised fashion. Our output, however, will be labels for each of the clouds, giving us predictions as to what digit they are meant to represent.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron\n",
    "\n",
    "Let's start with a kind of neural network we've seen before: a multi-layer perceptron. Recall from our previous neural networks sections that this is a set of perceptron models organized into layers, one layer feeding into the next.\n",
    "\n",
    "To do this, we will first need to reshape our data into flat vectors for each digit. We'll also need to convert our outcome to a matrix of binary variables, rather than the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Change shape \n",
    "# Note that our images are 28*28 pixels, so in reshaping to arrays we want\n",
    "# 60,000 arrays of length 784, one for each image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert to float32 for type consistency\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we can create our model. We'll do this using dense layers and dropouts. Dense layers are simply fully connected layers with a given number of perceptrons. Dropout drops a certain portion of our perceptrons in order to prevent overfitting. Our activation function, `relu` stands for Rectified Linear Unit, which is standard but can be read about more [here](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model. This we can use to accomplish our wildest dreams of data modeling, or at least predict some digits from pixel data. To do that we will use epochs, effectively iterations of the model, improving based on what it learned previously. Batch size is the number of samples to use in each step improving the model and will affect speed, but also slightly negatively impact accuracy (learning in bigger steps will affect what your model learns).\n",
    "\n",
    "Note that we are going with 64 perceptron wide layers, this is relatively arbitrary, though units within the $2^x$ series will parallelize more efficiently. Also note that our number of parameters is the product of our input width plus one and our layer width. This reflects the number of weights we're creating in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0309 - accuracy: 0.9908 - val_loss: 0.1170 - val_accuracy: 0.9796\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0348 - accuracy: 0.9897 - val_loss: 0.1167 - val_accuracy: 0.9791\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0323 - accuracy: 0.9903 - val_loss: 0.1270 - val_accuracy: 0.9789\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.1266 - val_accuracy: 0.9795\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0322 - accuracy: 0.9903 - val_loss: 0.1257 - val_accuracy: 0.9783\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0330 - accuracy: 0.9905 - val_loss: 0.1255 - val_accuracy: 0.9794\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0331 - accuracy: 0.9904 - val_loss: 0.1326 - val_accuracy: 0.9784\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0328 - accuracy: 0.9901 - val_loss: 0.1312 - val_accuracy: 0.9776\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0302 - accuracy: 0.9910 - val_loss: 0.1317 - val_accuracy: 0.9781\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.1250 - val_accuracy: 0.9773\n",
      "Test loss: 0.12495717712684491\n",
      "Test accuracy: 0.9772999882698059\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did impressively well for such a simple neural network, with each epoch training in about 1 second on this machine and giving us an accuracy in the high 90's. But what else can we do? Let's let our model get much more complicated by introducting convolution.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "Before we go any further, do you recall that we've talked about how complex neural networks can get, and the degree of computational complexity that entails? Well, here we're going to finally truly experience that complexity, so be careful about rerunning this code. It will take some serious time (potentially on the order of hours) to run.\n",
    "\n",
    "Now that that's out of the way, let's talk convolution. First, a simple definition. Convolution basically takes your data and creates overlapping subsegments testing for a given feature in a set of spaces and upon which it develops its model.\n",
    "\n",
    "Let's extend that definition since it's incredibly dense.\n",
    "\n",
    "First, you have to define a shape of your input data. This can theoretically be in any number of dimensions, though for our image example we will use 2d, since images are in two dimensions. This is also why you'll see 2D in some of our layer definitions (though more on that later). Our first chunk of code after loading the data does this reshaping (with a conditional on the data format).\n",
    "\n",
    "Over that shaped data, we then create our tiles, also called __kernels__. These kernels are like little windows, that will look over subsets of the data of a given size. In the example below we create 3x3 kernels, which run overlapping over the whole 28x28 input looking for features. That is the convolutional layer, a way of searching for a subpattern over the whole of the image. We can chain multiple of these convolutional layers together, with the below example having two.\n",
    "\n",
    "Next comes a pooling layer. This is a _downsampling_ technique, which effectively serves to reduce sample size and simplify later processes. For each value generated by our convolutional layers, it looks over the grid in _non_-overlapping segments and takes the maximum value of those outputs. It's not the feautres exact location then that matters, but its approximate or relative location. After pooling you will want to flatten the data back out, so that it can be put into dense layers as we did in MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.2534 - accuracy: 0.9210 - val_loss: 0.0554 - val_accuracy: 0.9816\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0848 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9863\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0639 - accuracy: 0.9814 - val_loss: 0.0389 - val_accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0541 - accuracy: 0.9840 - val_loss: 0.0326 - val_accuracy: 0.9884\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0477 - accuracy: 0.9854 - val_loss: 0.0320 - val_accuracy: 0.9884\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 0.0428 - accuracy: 0.9876 - val_loss: 0.0299 - val_accuracy: 0.9905\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.0373 - accuracy: 0.9889 - val_loss: 0.0267 - val_accuracy: 0.9907\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0341 - accuracy: 0.9895 - val_loss: 0.0287 - val_accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.0325 - accuracy: 0.9901 - val_loss: 0.0293 - val_accuracy: 0.9913\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0285 - accuracy: 0.9912 - val_loss: 0.0264 - val_accuracy: 0.9909\n",
      "Test loss: 0.026389713929603386\n",
      "Test accuracy: 0.9908999800682068\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is incredibly impressive accuracy. 99% is really exceptional, but it did take a long time to get there. Such are the costs of convolution.\n",
    "\n",
    "There is one more classic construction of a neural network: Recurrent, which we'll give quick mention.\n",
    "\n",
    "## Hierarchical Recurrrent Neural Networks\n",
    "\n",
    "So far when we've talked about neural networks we've talked about them as feedforward: data flows in one direction until it reaches the end. Recurrent neural networks do not obey that directional logic, instead letting the data cycle through the network.\n",
    "\n",
    "However, to do this we have to abandon the sequential model building we've done so far and things can get much more complicated. You have to use recurrent layers and often time distribution (which handles the extra dimension created through the LTSM layer, as a time dimension) to get these things running. You can find an example of a hierarchical recurrent network below (via the link [here](https://github.com/fchollet/keras/blob/master/examples/mnist_hierarchical_rnn.py)). When you get comfortable with networks as they exist in Keras for both convolution and MLP, start exploring recurrence. Note that this will take an even longer time than the previous ones should you choose to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 181s 3ms/step - loss: 0.9623 - accuracy: 0.6801 - val_loss: 0.3977 - val_accuracy: 0.8788\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 174s 3ms/step - loss: 0.3692 - accuracy: 0.8881 - val_loss: 0.2567 - val_accuracy: 0.9225\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 0.2507 - accuracy: 0.9258 - val_loss: 0.1813 - val_accuracy: 0.9474\n",
      "Test loss: 0.18129234880805015\n",
      "Test accuracy: 0.9473999738693237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be comfortable building some neural networks, but let's see if you can improve them!\n",
    "\n",
    "# Drill: 99% MLP\n",
    "\n",
    "We have the MLP above, which runs reasonably quickly. Copy that code down here and see if you can tune it to achieve 99% accuracy with a Multi-Layer Perceptron. Does it run faster than the recurrent or concolutional neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Remember to reload the data to get it in the format that an MLP expects. HRNN takes data in a different format, but it is still called x_train and y_train in this notebook.\n",
    "\n",
    "Our original MLP model had an out-of-sample accuracy of 0.977. First, the same model is estimated using 50 epochs instead of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.4398 - accuracy: 0.8718 - val_loss: 0.1956 - val_accuracy: 0.9398\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2038 - accuracy: 0.9387 - val_loss: 0.1389 - val_accuracy: 0.9559\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1545 - accuracy: 0.9538 - val_loss: 0.1132 - val_accuracy: 0.9656\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1279 - accuracy: 0.9613 - val_loss: 0.1048 - val_accuracy: 0.9694\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1118 - accuracy: 0.9659 - val_loss: 0.0909 - val_accuracy: 0.9731\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0998 - accuracy: 0.9686 - val_loss: 0.0981 - val_accuracy: 0.9717\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0935 - accuracy: 0.9717 - val_loss: 0.0876 - val_accuracy: 0.9736\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0824 - accuracy: 0.9751 - val_loss: 0.0863 - val_accuracy: 0.9758\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0796 - accuracy: 0.9753 - val_loss: 0.0849 - val_accuracy: 0.9762\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0752 - accuracy: 0.9768 - val_loss: 0.0835 - val_accuracy: 0.9767\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0708 - accuracy: 0.9785 - val_loss: 0.0856 - val_accuracy: 0.9758\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0681 - accuracy: 0.9791 - val_loss: 0.0854 - val_accuracy: 0.9781\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0644 - accuracy: 0.9797 - val_loss: 0.0860 - val_accuracy: 0.9752\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0635 - accuracy: 0.9802 - val_loss: 0.0860 - val_accuracy: 0.9755\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0589 - accuracy: 0.9811 - val_loss: 0.0886 - val_accuracy: 0.9766\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0567 - accuracy: 0.9821 - val_loss: 0.0947 - val_accuracy: 0.9744\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0544 - accuracy: 0.9822 - val_loss: 0.0866 - val_accuracy: 0.9773\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0546 - accuracy: 0.9834 - val_loss: 0.0898 - val_accuracy: 0.9771\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0535 - accuracy: 0.9832 - val_loss: 0.0875 - val_accuracy: 0.9764\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0504 - accuracy: 0.9838 - val_loss: 0.0862 - val_accuracy: 0.9791\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0497 - accuracy: 0.9845 - val_loss: 0.0904 - val_accuracy: 0.9787\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0482 - accuracy: 0.9846 - val_loss: 0.0931 - val_accuracy: 0.9755\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0477 - accuracy: 0.9852 - val_loss: 0.0955 - val_accuracy: 0.9786\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0463 - accuracy: 0.9853 - val_loss: 0.0969 - val_accuracy: 0.9775\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0445 - accuracy: 0.9855 - val_loss: 0.0974 - val_accuracy: 0.9793\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0461 - accuracy: 0.9849 - val_loss: 0.0928 - val_accuracy: 0.9782\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0428 - accuracy: 0.9868 - val_loss: 0.0984 - val_accuracy: 0.9782\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.0985 - val_accuracy: 0.9782\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0422 - accuracy: 0.9868 - val_loss: 0.0983 - val_accuracy: 0.9777\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.1105 - val_accuracy: 0.9768\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.1149 - val_accuracy: 0.9775\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.1063 - val_accuracy: 0.9784\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 0.1087 - val_accuracy: 0.9784\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0410 - accuracy: 0.9875 - val_loss: 0.1117 - val_accuracy: 0.9765\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0394 - accuracy: 0.9878 - val_loss: 0.1051 - val_accuracy: 0.9777\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0365 - accuracy: 0.9884 - val_loss: 0.1053 - val_accuracy: 0.9781\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0375 - accuracy: 0.9890 - val_loss: 0.1060 - val_accuracy: 0.9791\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0376 - accuracy: 0.9885 - val_loss: 0.1131 - val_accuracy: 0.9781\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.0377 - accuracy: 0.9883 - val_loss: 0.1152 - val_accuracy: 0.9775\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.1105 - val_accuracy: 0.9774\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0355 - accuracy: 0.9888 - val_loss: 0.1196 - val_accuracy: 0.9767\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0377 - accuracy: 0.9879 - val_loss: 0.1058 - val_accuracy: 0.9783\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.1150 - val_accuracy: 0.9788\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.1135 - val_accuracy: 0.9775\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0370 - accuracy: 0.9893 - val_loss: 0.1132 - val_accuracy: 0.9763\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0352 - accuracy: 0.9893 - val_loss: 0.1181 - val_accuracy: 0.9778\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0339 - accuracy: 0.9901 - val_loss: 0.1198 - val_accuracy: 0.9790\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0356 - accuracy: 0.9891 - val_loss: 0.1208 - val_accuracy: 0.9787\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0355 - accuracy: 0.9899 - val_loss: 0.1162 - val_accuracy: 0.9796\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.1163 - val_accuracy: 0.9795\n",
      "Test loss: 0.11633637395432561\n",
      "Test accuracy: 0.9794999957084656\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved slightly. Next, I increase the dropout parameter in each level. I also returned to 10 epochs for ease of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.5580 - accuracy: 0.8313 - val_loss: 0.2224 - val_accuracy: 0.9310\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2826 - accuracy: 0.9185 - val_loss: 0.1628 - val_accuracy: 0.9529\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2298 - accuracy: 0.9339 - val_loss: 0.1359 - val_accuracy: 0.9575\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2012 - accuracy: 0.9422 - val_loss: 0.1258 - val_accuracy: 0.9627\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1786 - accuracy: 0.9469 - val_loss: 0.1140 - val_accuracy: 0.9660\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1658 - accuracy: 0.9523 - val_loss: 0.1127 - val_accuracy: 0.9683\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1567 - accuracy: 0.9544 - val_loss: 0.1056 - val_accuracy: 0.9688\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1481 - accuracy: 0.9573 - val_loss: 0.1043 - val_accuracy: 0.9709\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1421 - accuracy: 0.9585 - val_loss: 0.1037 - val_accuracy: 0.9716\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.1384 - accuracy: 0.9599 - val_loss: 0.1023 - val_accuracy: 0.9727\n",
      "Test loss: 0.10228661063974723\n",
      "Test accuracy: 0.9726999998092651\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with decreased dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.3866 - accuracy: 0.8888 - val_loss: 0.1936 - val_accuracy: 0.9415\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1814 - accuracy: 0.9455 - val_loss: 0.1362 - val_accuracy: 0.9580\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1343 - accuracy: 0.9594 - val_loss: 0.1131 - val_accuracy: 0.9659\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1086 - accuracy: 0.9669 - val_loss: 0.1072 - val_accuracy: 0.9687\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0921 - accuracy: 0.9721 - val_loss: 0.1006 - val_accuracy: 0.9705\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0817 - accuracy: 0.9747 - val_loss: 0.0995 - val_accuracy: 0.9709\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0718 - accuracy: 0.9778 - val_loss: 0.0901 - val_accuracy: 0.9725\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0658 - accuracy: 0.9800 - val_loss: 0.0858 - val_accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0590 - accuracy: 0.9816 - val_loss: 0.0901 - val_accuracy: 0.9738\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0551 - accuracy: 0.9825 - val_loss: 0.0828 - val_accuracy: 0.9760\n",
      "Test loss: 0.08275505964832847\n",
      "Test accuracy: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.03))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.03))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither improved on our original accuracy rate. Try increasing the number of components in our first layer from 64 to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3633 - accuracy: 0.8938 - val_loss: 0.1491 - val_accuracy: 0.9520\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1605 - accuracy: 0.9512 - val_loss: 0.1132 - val_accuracy: 0.9638\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1176 - accuracy: 0.9646 - val_loss: 0.0975 - val_accuracy: 0.9695\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0948 - accuracy: 0.9714 - val_loss: 0.0881 - val_accuracy: 0.9743\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.0819 - accuracy: 0.9753 - val_loss: 0.0891 - val_accuracy: 0.9737\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0716 - accuracy: 0.9785 - val_loss: 0.0764 - val_accuracy: 0.9774\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 0.0883 - val_accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0561 - accuracy: 0.9824 - val_loss: 0.0812 - val_accuracy: 0.9770\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.0517 - accuracy: 0.9839 - val_loss: 0.0732 - val_accuracy: 0.9795\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.0477 - accuracy: 0.9855 - val_loss: 0.0707 - val_accuracy: 0.9811\n",
      "Test loss: 0.0707141342728486\n",
      "Test accuracy: 0.9811000227928162\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increased further to 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3129 - accuracy: 0.9093 - val_loss: 0.1616 - val_accuracy: 0.9499\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1287 - accuracy: 0.9607 - val_loss: 0.0926 - val_accuracy: 0.9716\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0915 - accuracy: 0.9716 - val_loss: 0.0750 - val_accuracy: 0.9765\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0704 - accuracy: 0.9785 - val_loss: 0.0745 - val_accuracy: 0.9775\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0584 - accuracy: 0.9819 - val_loss: 0.0770 - val_accuracy: 0.9797\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0483 - accuracy: 0.9848 - val_loss: 0.0749 - val_accuracy: 0.9798\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.0660 - val_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0360 - accuracy: 0.9885 - val_loss: 0.0731 - val_accuracy: 0.9801\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.0763 - val_accuracy: 0.9805\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0663 - val_accuracy: 0.9837\n",
      "Test loss: 0.0662787317172988\n",
      "Test accuracy: 0.9836999773979187\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a little bit better. Let's try increasing the size of the third layer from 64 to 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2901 - accuracy: 0.9136 - val_loss: 0.1267 - val_accuracy: 0.9627\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1199 - accuracy: 0.9640 - val_loss: 0.0953 - val_accuracy: 0.9711\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0859 - accuracy: 0.9740 - val_loss: 0.0828 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0641 - accuracy: 0.9803 - val_loss: 0.0747 - val_accuracy: 0.9794\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0736 - val_accuracy: 0.9799\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0453 - accuracy: 0.9855 - val_loss: 0.0767 - val_accuracy: 0.9787\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0382 - accuracy: 0.9882 - val_loss: 0.0766 - val_accuracy: 0.9801\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0328 - accuracy: 0.9892 - val_loss: 0.0827 - val_accuracy: 0.9814\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0304 - accuracy: 0.9902 - val_loss: 0.0869 - val_accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0270 - accuracy: 0.9916 - val_loss: 0.0804 - val_accuracy: 0.9806\n",
      "Test loss: 0.08042626062917407\n",
      "Test accuracy: 0.9805999994277954\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-sample accuracy decreased a little bit. Let's see if decreasing the size of the second layer has any effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3745 - accuracy: 0.8904 - val_loss: 0.1433 - val_accuracy: 0.9572\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1473 - accuracy: 0.9566 - val_loss: 0.1063 - val_accuracy: 0.9676\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1038 - accuracy: 0.9688 - val_loss: 0.0893 - val_accuracy: 0.9717\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0815 - accuracy: 0.9755 - val_loss: 0.0900 - val_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0677 - accuracy: 0.9791 - val_loss: 0.0738 - val_accuracy: 0.9782\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0589 - accuracy: 0.9821 - val_loss: 0.0665 - val_accuracy: 0.9787\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.0676 - val_accuracy: 0.9802\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0457 - accuracy: 0.9858 - val_loss: 0.0661 - val_accuracy: 0.9808\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0385 - accuracy: 0.9881 - val_loss: 0.0720 - val_accuracy: 0.9794\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0355 - accuracy: 0.9894 - val_loss: 0.0705 - val_accuracy: 0.9809\n",
      "Test loss: 0.07053206363034333\n",
      "Test accuracy: 0.98089998960495\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, still decreased. Looks like our second layer is pretty well tuned. Let's try widening our first layer even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.2736 - accuracy: 0.9183 - val_loss: 0.1253 - val_accuracy: 0.9608\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1064 - accuracy: 0.9681 - val_loss: 0.0863 - val_accuracy: 0.9733\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.0753 - accuracy: 0.9768 - val_loss: 0.0762 - val_accuracy: 0.9768\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.0578 - accuracy: 0.9821 - val_loss: 0.0772 - val_accuracy: 0.9779\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0454 - accuracy: 0.9860 - val_loss: 0.0684 - val_accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.0369 - accuracy: 0.9886 - val_loss: 0.0728 - val_accuracy: 0.9817\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.0717 - val_accuracy: 0.9818\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0279 - accuracy: 0.9913 - val_loss: 0.0631 - val_accuracy: 0.9844\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.0737 - val_accuracy: 0.9826\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0832 - val_accuracy: 0.9807\n",
      "Test loss: 0.08315340326343285\n",
      "Test accuracy: 0.9807000160217285\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was starting to take a little longer to train, but this jump is when it was really felt (about 5s per epoch compared to less than 2 in our first model). Regardless, this set of parameters did worse, so I moved on assuming 256 was a fairly optimized parameter for the size of the first layer.\n",
    "\n",
    "Next I played around with different loss functions. Most did not improve the model, but \"binary crossentropy,\" did, and we finally achieved >99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0529 - accuracy: 0.9819 - val_loss: 0.0247 - val_accuracy: 0.9917\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.0188 - val_accuracy: 0.9935\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0147 - val_accuracy: 0.9950\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0140 - val_accuracy: 0.9952\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0142 - val_accuracy: 0.9954\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0123 - val_accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0145 - val_accuracy: 0.9954\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0131 - val_accuracy: 0.9960\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0066 - accuracy: 0.9978 - val_loss: 0.0125 - val_accuracy: 0.9962\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0131 - val_accuracy: 0.9963\n",
      "Test loss: 0.013149157796365399\n",
      "Test accuracy: 0.9963293671607971\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For curiosities sake, I also tested to see if using the same loss function in a CNN produced better resultes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0676 - accuracy: 0.9765 - val_loss: 0.0160 - val_accuracy: 0.9946\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0240 - accuracy: 0.9921 - val_loss: 0.0099 - val_accuracy: 0.9965\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.0092 - val_accuracy: 0.9966\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0073 - val_accuracy: 0.9973\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0070 - val_accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.0063 - val_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0061 - val_accuracy: 0.9977\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0063 - val_accuracy: 0.9976\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0058 - val_accuracy: 0.9979\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0055 - val_accuracy: 0.9981\n",
      "Test loss: 0.00549634813297996\n",
      "Test accuracy: 0.9980797171592712\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions, from our data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN with the categorical crossentropy loss function achieved 99.1% accuracy. With binary crossentropy, the CNN achieved 99.5% accuracy after just the first epoch, and after 10 epochs (and about 15 minutes), it achieved 99.8% accuracy."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
