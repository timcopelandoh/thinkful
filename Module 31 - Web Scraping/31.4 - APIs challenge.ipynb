{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built a web scraper that scrapes the academic Economics forum \"EconJobRumors,\" and returns a list of names of discussion topics, and the corresponding links. The number of pages is adjustable, but I scraped from the first 20 pages. The script takes about 20 seconds to run.\n",
    "\n",
    "Note: I did not use an API, but scraped from the page directly. The prompt asks to \"Do a little scraping or API-calling of your own,\" so I don't anticipate this being a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "num_pages = 20\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.econjobrumors.com',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for page in response.xpath('//tr'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "\n",
    "                'name': page.xpath('td/a[starts-with(@href, \"https://www.econjobrumors.com/topic/\")]/text()').extract_first(),\n",
    "                'link': page.xpath('td/a[starts-with(@href, \"https://www.econjobrumors.com/topic/\")]/@href').extract_first(),\n",
    "\n",
    "            }\n",
    "        # Find current page number\n",
    "        page_num = int(response.xpath('//span[@class=\"page-numbers current\"]/text()').extract_first())\n",
    "        \n",
    "        print(page_num)\n",
    "\n",
    "        if page_num <= num_pages:\n",
    "        \tnext_page = 'https://www.econjobrumors.com/page/' + str(page_num+1)\n",
    "        \tyield scrapy.Request(next_page, callback = self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'json_output.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove null values and print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.econjobrumors.com/topic/request-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5 months</td>\n",
       "      <td>https://www.econjobrumors.com/topic/about-ejmr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Universities withdrawing offers</td>\n",
       "      <td>https://www.econjobrumors.com/topic/universiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Dow Jones would surely touch 17000 once the ne...</td>\n",
       "      <td>https://www.econjobrumors.com/topic/dow-jones-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Bill Ackman is buying stocks.</td>\n",
       "      <td>https://www.econjobrumors.com/topic/bill-ackma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "25                                                  2   \n",
       "26                                           5 months   \n",
       "27                    Universities withdrawing offers   \n",
       "28  Dow Jones would surely touch 17000 once the ne...   \n",
       "29                      Bill Ackman is buying stocks.   \n",
       "\n",
       "                                                 link  \n",
       "25  https://www.econjobrumors.com/topic/request-a-...  \n",
       "26  https://www.econjobrumors.com/topic/about-ejmr...  \n",
       "27  https://www.econjobrumors.com/topic/universiti...  \n",
       "28  https://www.econjobrumors.com/topic/dow-jones-...  \n",
       "29  https://www.econjobrumors.com/topic/bill-ackma...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pages = pd.read_json('json_output.json')\n",
    "pages = pages[(pages.name.isna() == False)]\n",
    "\n",
    "pages.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
